1ba6f9b32f9dc038b551fdba824abae0c561f4c4

import torch
from torch import Tensor
from protomotions.envs.base_env.env import BaseEnv

class Parkour(BaseEnv):
    def __init__(self, config, device: torch.device, *args, **kwargs):
        super().__init__(config=config, device=device, *args, **kwargs)

        # Same as steering
        self.parkour_obs = torch.zeros(
            (self.config.num_envs, self.config.steering_params.obs_size),
            device=device,
            dtype=torch.float,
        )

        # Track starting position
        self._start_pos = torch.zeros([self.num_envs, 3], device=self.device, dtype=torch.float)

    def reset(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=self.device, dtype=torch.long)
        
        obs = super().reset(env_ids)
        
        # Remember where we started
        self._start_pos[env_ids] = self.simulator.get_root_state(env_ids).root_pos
        
        return obs

    def compute_observations(self, env_ids=None):
        super().compute_observations(env_ids)

        if env_ids is None:
            root_pos = self.simulator.get_root_state().root_pos
            start_pos = self._start_pos
        else:
            root_pos = self.simulator.get_root_state(env_ids).root_pos
            start_pos = self._start_pos[env_ids]

        # Simple observation: how far forward we've moved
        forward_progress = (root_pos - start_pos)[..., 0]  # x-direction
        goal_distance = 2.0 - forward_progress  # distance left to go
        
        # 4 observations: [progress, distance_left, 0, 0]
        obs = torch.stack([
            forward_progress,
            goal_distance,
            torch.zeros_like(forward_progress),
            torch.zeros_like(forward_progress)
        ], dim=-1)

        self.parkour_obs[env_ids] = obs

    def get_obs(self):
        obs = super().get_obs()
        obs.update({"parkour": self.parkour_obs})
        return obs

    def compute_reward(self):
        root_pos = self.simulator.get_root_state().root_pos
        
        # How far forward from start
        forward_progress = (root_pos - self._start_pos)[..., 0]
        
        # Simple reward: closer to 2 meters = better
        self.rew_buf[:] = 2.0 - torch.abs(2.0 - forward_progress)
        
        # Success bonus
        reached_goal = forward_progress >= 2.0
        self.rew_buf[:] += reached_goal.float() * 10.0

    def is_done(self):
        root_pos = self.simulator.get_root_state().root_pos
        forward_progress = (root_pos - self._start_pos)[..., 0]
        
        # Done when: reached 2m goal OR fell down
        success = forward_progress >= 2.0
        fallen = root_pos[..., 2] < 0.3
        
        return success | fallen
